{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fabc74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"X:/ML Projects/News RAG/data/pdfs\")   # <- put your folder here\n",
    "ARTIFACTS = Path(\"./artifacts\"); ARTIFACTS.mkdir(exist_ok=True)\n",
    "\n",
    "# chunk sizes tuned for history/politics prose\n",
    "FINE_CHUNK_CHARS   = 1800   # ~450 tokens\n",
    "FINE_OVERLAP_CHARS = 300    # ~75 tokens\n",
    "COARSE_MIN_PAGES   = 5      # fallback “chapter” size when TOC missing\n",
    "\n",
    "EMB_MODEL_TEXT  = \"BAAI/bge-large-en-v1.5\"    # passages\n",
    "EMB_MODEL_QUERY = \"BAAI/bge-large-en-v1.5\"    # queries (bge works for both)\n",
    "RERANK_MODEL    = \"BAAI/bge-reranker-large\"   # cross-encoder for re-ranking\n",
    "\n",
    "INDEX_DIM = 1024  # bge-large-en is 1024-d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affdcde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz, json, hashlib, re\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "@dataclass\n",
    "class DocMeta:\n",
    "    doc_id: str\n",
    "    title: str\n",
    "    author: Optional[str]\n",
    "    source_path: str\n",
    "    pages: int\n",
    "    creationdate: Optional[str]\n",
    "    moddate: Optional[str]\n",
    "\n",
    "@dataclass\n",
    "class CoarseSeg:\n",
    "    coarse_id: str\n",
    "    doc_id: str\n",
    "    title: str         # chapter/headline or synthetic\n",
    "    page_start: int\n",
    "    page_end: int\n",
    "    text: str\n",
    "\n",
    "def pdf_sha(path: str) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1<<20), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()[:16]\n",
    "\n",
    "def extract_pdf(path: Path):\n",
    "    doc = fitz.open(path)\n",
    "    md = doc.metadata or {}\n",
    "    meta = DocMeta(\n",
    "        doc_id = pdf_sha(str(path)),\n",
    "        title  = md.get(\"title\") or path.stem,\n",
    "        author = md.get(\"author\"),\n",
    "        source_path = str(path),\n",
    "        pages  = len(doc),\n",
    "        creationdate = md.get(\"creationDate\"),\n",
    "        moddate = md.get(\"modDate\"),\n",
    "    )\n",
    "    # full text per page\n",
    "    pages = [doc[i].get_text(\"text\") for i in range(len(doc))]\n",
    "    # try TOC\n",
    "    toc = doc.get_toc(simple=True)  # list of [level, title, page]\n",
    "    coarse: List[CoarseSeg] = []\n",
    "    if toc:\n",
    "        # convert TOC to page spans\n",
    "        entries = []\n",
    "        for i, (lvl, title, page1) in enumerate(toc):\n",
    "            page0 = max(page1-1, 0)\n",
    "            page2 = (toc[i+1][2]-1) if i+1 < len(toc) else len(doc)-1\n",
    "            if page2 < page0: continue\n",
    "            entries.append((title.strip(), page0, page2))\n",
    "        for title, p0, p1 in entries:\n",
    "            text = \"\\n\".join(pages[p0:p1+1]).strip()\n",
    "            if not text: continue\n",
    "            coarse.append(CoarseSeg(\n",
    "                coarse_id=f\"{meta.doc_id}:c:{p0}-{p1}\",\n",
    "                doc_id=meta.doc_id, title=title, page_start=p0, page_end=p1, text=text\n",
    "            ))\n",
    "    else:\n",
    "        # fallback coarse segments by fixed windows\n",
    "        p = 0\n",
    "        while p < len(pages):\n",
    "            q = min(p+COARSE_MIN_PAGES-1, len(pages)-1)\n",
    "            text = \"\\n\".join(pages[p:q+1]).strip()\n",
    "            if text:\n",
    "                coarse.append(CoarseSeg(\n",
    "                    coarse_id=f\"{meta.doc_id}:c:{p}-{q}\",\n",
    "                    doc_id=meta.doc_id, title=f\"Section p{p+1}-{q+1}\", page_start=p, page_end=q, text=text\n",
    "                ))\n",
    "            p = q+1\n",
    "    doc.close()\n",
    "    return meta, pages, coarse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4425f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_recursive(text: str, max_chars=FINE_CHUNK_CHARS, overlap=FINE_OVERLAP_CHARS):\n",
    "    seps = [\"\\n\\n\", \"\\n\", \". \"]\n",
    "    chunks = [text]\n",
    "    for sep in seps:\n",
    "        new = []\n",
    "        for c in chunks:\n",
    "            if len(c) <= max_chars: new.append(c); continue\n",
    "            parts = c.split(sep)\n",
    "            buf = \"\"\n",
    "            for p in parts:\n",
    "                piece = (p + (sep if c.find(sep)>=0 else \"\"))\n",
    "                if len(buf) + len(piece) > max_chars and buf:\n",
    "                    new.append(buf.strip())\n",
    "                    # start new with overlap tail\n",
    "                    buf = buf[-overlap:] + piece\n",
    "                else:\n",
    "                    buf += piece\n",
    "            if buf.strip():\n",
    "                new.append(buf.strip())\n",
    "        chunks = new\n",
    "    # final hard wrap\n",
    "    final=[]\n",
    "    for c in chunks:\n",
    "        s=0\n",
    "        while s < len(c):\n",
    "            final.append(c[s:s+max_chars])\n",
    "            s += (max_chars - overlap)\n",
    "    return [x.strip() for x in final if x.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e39c6f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 87/87 [01:29<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 29 | Coarse segs: 5397 | Fine chunks: 153375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def build_corpora(pdf_dir: Path):\n",
    "    docs_meta: Dict[str, DocMeta] = {}\n",
    "    coarse_segments: List[CoarseSeg] = []\n",
    "    fine_chunks = []   # dicts with metadata\n",
    "\n",
    "    for pdf in tqdm(sorted(pdf_dir.glob(\"**/*.pdf\"))):\n",
    "        meta, pages, coarse = extract_pdf(pdf)\n",
    "        docs_meta[meta.doc_id] = meta\n",
    "        coarse_segments.extend(coarse)\n",
    "        # fine chunks (anchored in coarse)\n",
    "        for seg in coarse:\n",
    "            for i, chunk in enumerate(split_recursive(seg.text)):\n",
    "                fine_chunks.append({\n",
    "                    \"fine_id\": f\"{seg.coarse_id}:f:{i}\",\n",
    "                    \"doc_id\": meta.doc_id,\n",
    "                    \"coarse_id\": seg.coarse_id,\n",
    "                    \"title\": seg.title,\n",
    "                    \"page_start\": seg.page_start,\n",
    "                    \"page_end\": seg.page_end,\n",
    "                    \"text\": chunk,\n",
    "                    # room for future enrichment\n",
    "                    \"year\": None, \"author\": meta.author, \"source_path\": meta.source_path\n",
    "                })\n",
    "    return docs_meta, coarse_segments, fine_chunks\n",
    "\n",
    "docs_meta, coarse_segments, fine_chunks = build_corpora(DATA_DIR)\n",
    "\n",
    "print(f\"Docs: {len(docs_meta)} | Coarse segs: {len(coarse_segments)} | Fine chunks: {len(fine_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "390d5ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1107 15:54:20.280000 25184 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0871a78ab5d24df58b0d95d24ac9bc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f5beb6ec6a4a49b697b3552aa2ff3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c240352aad413c9de19169d95110ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760ba6216cc14c2b88f6f36af76d0516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b09e74a05e4b9a94b687faa6b8a261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0973901bd5e14a928c83067dd32a4432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "PASSAGE_PREFIX = \"passage: \"\n",
    "QUERY_PREFIX   = \"query: \"\n",
    "\n",
    "emb_text = SentenceTransformer(EMB_MODEL_TEXT, device=\"cpu\")   # switch to 'cuda' if available\n",
    "\n",
    "def embed_passages(texts: list[str]) -> np.ndarray:\n",
    "    return emb_text.encode([PASSAGE_PREFIX + t for t in texts],\n",
    "                           normalize_embeddings=True, batch_size=64, convert_to_numpy=True)\n",
    "\n",
    "def embed_queries(texts: list[str]) -> np.ndarray:\n",
    "    return emb_text.encode([QUERY_PREFIX + t for t in texts],\n",
    "                           normalize_embeddings=True, batch_size=16, convert_to_numpy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f67ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "# from rapidfuzz.string_metric import levenshtein\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    # simple tokenizer for BM25 (can swap to nltk/spacy later)\n",
    "    return re.findall(r\"[A-Za-z0-9\\-]+\", text.lower())\n",
    "\n",
    "# ---- Coarse index ----\n",
    "coarse_texts = [c.text for c in coarse_segments]\n",
    "coarse_vecs  = embed_passages(coarse_texts).astype('float32')\n",
    "index_coarse = faiss.IndexHNSWFlat(INDEX_DIM, 64)  # M=64\n",
    "index_coarse.hnsw.efConstruction = 200\n",
    "index_coarse.add(coarse_vecs)\n",
    "\n",
    "bm25_coarse = BM25Okapi([tokenize(t) for t in coarse_texts])\n",
    "\n",
    "# ---- Fine index ----\n",
    "fine_texts = [c[\"text\"] for c in fine_chunks]\n",
    "fine_vecs  = embed_passages(fine_texts).astype('float32')\n",
    "index_fine = faiss.IndexHNSWFlat(INDEX_DIM, 64)\n",
    "index_fine.hnsw.efConstruction = 200\n",
    "index_fine.add(fine_vecs)\n",
    "\n",
    "bm25_fine = BM25Okapi([tokenize(t) for t in fine_texts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dce736a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rapidfuzz in x:\\ml projects\\news rag\\src\\venv\\lib\\site-packages (3.14.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35568bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
